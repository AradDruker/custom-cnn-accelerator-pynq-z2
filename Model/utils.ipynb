{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "tensor([[[[0.1541]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Average time taken for convolution on CPU: 43033.40 nanoseconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "# Specify device as CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define the input tensor (batch_size=1, channels=1, height=5, width=5)\n",
    "input_tensor = torch.randn(1, 1, 5, 5).to(device)\n",
    "\n",
    "# Define a convolutional layer with a 5x5 kernel and move it to the CPU\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=5, bias=False).to(device)\n",
    "\n",
    "# Warm-up (important to stabilize timings)\n",
    "for _ in range(10):\n",
    "    _ = conv_layer(input_tensor)\n",
    "\n",
    "# Run multiple iterations and measure time\n",
    "num_iterations = 1000\n",
    "start_time = time.time_ns()  # Start time in nanoseconds\n",
    "for _ in range(num_iterations):\n",
    "    output = conv_layer(input_tensor)\n",
    "end_time = time.time_ns()  # End time in nanoseconds\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time_ns = (end_time - start_time) / num_iterations  # Average time per iteration\n",
    "\n",
    "print(f\"Output:\\n{output}\")\n",
    "print(f\"Average time taken for convolution on CPU: {elapsed_time_ns:.2f} nanoseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5x5 Kernel Centered on First Pixel:\n",
      "tensor([[  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [255, 255, 255,   0,   0],\n",
      "        [255, 255, 255,   0,   0],\n",
      "        [255, 255, 255,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given 28x28 tensor (replace with your actual tensor)\n",
    "tensor_28x28 = torch.tensor([[255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           211, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 136, 137,  54,\n",
    "           211, 138,  72, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 217,  59,  44,   7,\n",
    "            21,  11,   4,  11,  16,  16, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 170, 170,  59,   7,  15,\n",
    "            13,  53, 105,   2,   2,   8, 126, 210, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 152,  29,   0,  51, 161,\n",
    "           186, 186, 215, 125, 118,   8,  19,  57, 253, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 181,  37,   0,  65, 176,\n",
    "           252, 255, 224, 221, 188,  13,   7,   7, 154, 248, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 235,  85,  17,  35,  35, 126,\n",
    "           255, 255, 255, 255, 252,  71,  10,  27, 180, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 196,  84,   1,  40, 222,\n",
    "           254, 255, 255, 255, 250, 120,  26,  27, 123, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  84,  23,   7, 123,\n",
    "           199, 219, 251, 194,  67,  24,  14,  96,  96, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 244, 138,  46,   1,   7,\n",
    "            37,  95, 193,  77,  67,   7,  37, 102, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 235, 118, 164,  60,   1,   0,\n",
    "             0,  35,   6,   2,  16,  10,  85, 138, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 183, 116,  33,   4,  38,   9,   3,   0,\n",
    "             0,   0,   0,  10,  53, 162, 253, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 114,  45,   4,  20,  15,  76,  28, 115,\n",
    "           151,   0,   0,   1, 121, 224, 253, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255,  86,  86,  22,   0,  20,  96, 196, 177, 225,\n",
    "           235,  73,  17,   0,  68, 173, 251, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 112,  21,   0,  99, 183, 252, 252, 244, 255,\n",
    "           255, 213, 143,   0,   9,  54, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 186,  21,   0,  57, 210, 255, 255, 255, 255,\n",
    "           255, 230, 230,  52,   2,   9, 154, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 155,   9,   9,   0, 140, 200, 219, 255, 255,\n",
    "           255, 255, 167,  41,   0,  32, 222, 222, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255,  62,  22,  12,  73, 114, 222, 213, 213,\n",
    "           255, 229, 124,  56,   0,  29, 170, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 220, 148,  12,   5,   1, 121, 101,  51,\n",
    "           134,  50,   1,   1,   1,  29, 155, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255,  32,  32,   9,   1,   4,   7,\n",
    "            12,   1,   1,  17,  89, 194, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  24,  50, 105,\n",
    "            16,  31,  16,  77, 186, 241, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 105,\n",
    "           180, 236, 185, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "          [255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
    "           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255]])\n",
    "\n",
    "# Step 1: Add padding of 2 to the tensor\n",
    "padded_tensor = F.pad(tensor_28x28, pad=(2, 2, 2, 2), mode='constant', value=0)\n",
    "\n",
    "# Step 2: Extract the 5x5 kernel around the first pixel\n",
    "# The first pixel (0, 0) in the original tensor corresponds to (2, 2) in the padded tensor\n",
    "kernel_5x5 = padded_tensor[0:5, 27:32]  # Centered at (2, 2) in padded tensor\n",
    "\n",
    "# Print the result\n",
    "print(\"5x5 Kernel Centered on First Pixel:\")\n",
    "print(kernel_5x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias quant (Quantized): 8547\n",
      "Output (With Bias): 8951\n",
      "Quantized Output (8-bit): 26\n",
      "M scale: 0.0029076067472083695\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Quantized input tensor (5x5 region in uint8)\n",
    "xq = torch.tensor([\n",
    "    [  0,   0,   0,   0, 0],\n",
    "    [ 0,   0,   0,   0, 0],\n",
    "    [   0,   0, 254, 254, 254],\n",
    "    [  0,   0,254, 254, 254],\n",
    "    [0,   0, 254, 254, 254],\n",
    "], dtype=torch.uint8)  # Shape: (5, 5)\n",
    "\n",
    "# Quantized weights (5x5 kernel in int8)\n",
    "wq = torch.tensor([\n",
    "    [-9,   48,   20,   54,  -16],\n",
    "    [ 74,   72,   17,   34,  -11],\n",
    "    [-15,   13,-3,  -18,   42],\n",
    "    [   -98,  -65,   13,  -37,    7],\n",
    "    [-84, -127,  -31,   17,25],\n",
    "], dtype=torch.int8)  # Shape: (5, 5)\n",
    "\n",
    "# Quantization parameters\n",
    "input_scale = 0.0078\n",
    "input_zero_point = 127\n",
    "\n",
    "output_scale = 0.0108  # Scale for input activations\n",
    "output_zero_point = 0  # Zero-point for input activations\n",
    "\n",
    "weight_scale = 0.004025917034596205  # Scale for weights\n",
    "weight_zero_point = 0  # Zero-point for weights\n",
    "\n",
    "bias_float = 0.2684  # Bias in floating-point\n",
    "effective_scale = input_scale * weight_scale # Scale for the output\n",
    "\n",
    "xq_signed = xq.to(torch.int16) - input_zero_point  # Shift to zero-centered\n",
    "\n",
    "z = torch.sum(xq_signed.to(torch.int8) * wq.to(torch.int8)).item()\n",
    "\n",
    "bias_q  = round(bias_float / effective_scale) # Bias in quantized form\n",
    "print(\"Bias quant (Quantized):\", bias_q)\n",
    "\n",
    "z_int = z + bias_q\n",
    "\n",
    "# Step 4: Quantize the result back to 8-bit (uint8)\n",
    "M = effective_scale / output_scale\n",
    "z_out = round(z_int * M - output_zero_point)  # Quantized output\n",
    "\n",
    "# Step 5: Clamp to the valid uint8 range [0, 255]s\n",
    "output_quantized = max(0, min(255, z_out))\n",
    "\n",
    "# Display results\n",
    "print(\"Output (With Bias):\", z_int)\n",
    "print(\"Quantized Output (8-bit):\", output_quantized)\n",
    "print(\"M scale:\", M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create coe files for conv1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coe_file_hex_with_padding(numbers1, numbers2, numbers3, numbers4, numbers5, numbers6, numbers7, numbers8, numbers9, numbers10, numbers11, numbers12,output_file):\n",
    "    \"\"\"\n",
    "    Generate a COE file with two concatenated hexadecimal numbers.\n",
    "    Each list is processed as follows:\n",
    "      - All but the last number are represented as 8-bit signed binary values (2's complement).\n",
    "      - The last number is represented as a 32-bit signed binary value (2's complement).\n",
    "\n",
    "    The output file will have:\n",
    "      memory_initialization_radix=16;\n",
    "      memory_initialization_vector=\n",
    "      <hex_for_1_array> <hex_for_2_array> <hex_for_3_array> <hex_for_4_array> <hex_for_5_array> <hex_for_6_array>;\n",
    "\n",
    "    Parameters:\n",
    "        numbers1 (list of int): The first input array of signed integers.\n",
    "        numbers2 (list of int): The second input array of signed integers.\n",
    "        numbers3 (list of int): The second input array of signed integers.\n",
    "        numbers4 (list of int): The second input array of signed integers.\n",
    "        numbers5 (list of int): The second input array of signed integers.\n",
    "        numbers6 (list of int): The second input array of signed integers.\n",
    "        output_file (str): The name of the output COE file.\n",
    "    \"\"\"\n",
    "    def process_numbers(numbers):\n",
    "        if not isinstance(numbers, list) or not all(isinstance(n, int) for n in numbers):\n",
    "            raise ValueError(\"Each input must be a list of signed integers.\")\n",
    "\n",
    "        if len(numbers) < 2:\n",
    "            raise ValueError(\"Each input list must contain at least two numbers.\")\n",
    "\n",
    "        binary_values = []\n",
    "\n",
    "        # Process all numbers except the last as 8-bit signed binary\n",
    "        for num in numbers[:-1]:\n",
    "            if num < -128 or num > 127:\n",
    "                raise ValueError(f\"Number {num} exceeds the range of an 8-bit signed integer (-128 to 127).\")\n",
    "            binary = f\"{num & 0xFF:08b}\"  # 8-bit 2's complement\n",
    "            binary_values.append(binary)\n",
    "\n",
    "        # Process the last number as 32-bit signed binary\n",
    "        last_num = numbers[-1]\n",
    "        if last_num < -2**31 or last_num > 2**31 - 1:\n",
    "            raise ValueError(f\"Last number {last_num} exceeds the range of a 32-bit signed integer.\")\n",
    "        last_binary = f\"{last_num & 0xFFFFFFFF:032b}\"\n",
    "\n",
    "        # Combine all binary values\n",
    "        concatenated_binary = \"\".join(binary_values) + last_binary\n",
    "\n",
    "        # Convert binary string to hexadecimal\n",
    "        concatenated_hex = f\"{int(concatenated_binary, 2):X}\".zfill(len(concatenated_binary) // 4)\n",
    "        return concatenated_hex\n",
    "\n",
    "    # Process both arrays\n",
    "    hex1 = process_numbers(numbers1)\n",
    "    hex2 = process_numbers(numbers2)\n",
    "    hex3 = process_numbers(numbers3)\n",
    "    hex4 = process_numbers(numbers4)\n",
    "    hex5 = process_numbers(numbers5)\n",
    "    hex6 = process_numbers(numbers6)\n",
    "    hex7 = process_numbers(numbers7)\n",
    "    hex8 = process_numbers(numbers8)\n",
    "    hex9 = process_numbers(numbers9)\n",
    "    hex10 = process_numbers(numbers10)\n",
    "    hex11 = process_numbers(numbers11)\n",
    "    hex12 = process_numbers(numbers12)\n",
    "\n",
    "\n",
    "    # Write to COE file\n",
    "    with open(output_file, \"w\") as coe_file:\n",
    "        coe_file.write(\"memory_initialization_radix=16;\\n\")\n",
    "        coe_file.write(\"memory_initialization_vector=\\n\")\n",
    "        coe_file.write(f\"{hex1} {hex2} {hex3} {hex4} {hex5} {hex6} {hex7} {hex8} {hex9} {hex10} {hex11} {hex12};\\n\")\n",
    "\n",
    "    return hex1, hex2, hex3, hex4, hex5, hex6, hex7, hex8, hex9, hex10, hex11, hex12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "import torch.quantization as quant\n",
    "import warnings\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*weights_only=False.*\",  # Match the specific warning message\n",
    "    category=FutureWarning\n",
    ")\n",
    "\n",
    "def save_tensor_to_csv_as_matrix(tensor, filename):\n",
    "    \"\"\"\n",
    "    Save a PyTorch tensor to a CSV file, formatted as matrices for each channel.\n",
    "    Each matrix represents the spatial dimensions (Height x Width) for a channel,\n",
    "    with rows separated by commas.\n",
    "\n",
    "    Parameters:\n",
    "        tensor (torch.Tensor): The tensor to save (e.g., Batch x Channels x Height x Width).\n",
    "        filename (str): The name of the CSV file to create.\n",
    "    \"\"\"\n",
    "    # Handle quantized tensors\n",
    "    if tensor.is_quantized:\n",
    "        tensor = tensor.int_repr()  # Extract the integer representation\n",
    "\n",
    "    # Convert the tensor to a NumPy array\n",
    "    numpy_array = tensor.detach().cpu().numpy()\n",
    "\n",
    "    # Ensure the tensor has 4 dimensions: [Batch, Channels, Height, Width]\n",
    "    if len(numpy_array.shape) != 4:\n",
    "        raise ValueError(\"Tensor must have 4 dimensions (Batch x Channels x Height x Width).\")\n",
    "\n",
    "    batch_size, num_channels, height, width = numpy_array.shape\n",
    "\n",
    "    if batch_size != 1:\n",
    "        raise ValueError(\"Only batch size of 1 is supported.\")\n",
    "\n",
    "    # Open the file and write matrices for each channel\n",
    "    with open(filename, mode=\"w\") as file:\n",
    "        for channel in range(num_channels):\n",
    "            file.write(f\"Channel {channel + 1}:\\n\")  # Channel header\n",
    "            for i, row in enumerate(numpy_array[0, channel]):  # Iterate over height (rows)\n",
    "                file.write(f\"{list(row)}\")  # Format each row as a list\n",
    "                if i < height - 1:\n",
    "                    file.write(\",\\n\")  # Add a comma between rows\n",
    "                else:\n",
    "                    file.write(\"\\n\")  # No comma after the last row\n",
    "            file.write(\"\\n\")  # Add a newline between channels\n",
    "\n",
    "    print(f\"Tensor saved to {filename} in matrix format.\")\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=33):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        self.print_activation = False  # Add a flag to control printing\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        if self.print_activation:\n",
    "            # print(\"quant output shape:\", x)\n",
    "            # save_tensor_to_csv_as_matrix(x, \"image_output.csv\")\n",
    "            pass\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        if self.print_activation:\n",
    "            # print(\"Conv1 output shape:\", x.int_repr())\n",
    "            # save_tensor_to_csv_as_matrix(x, \"relu1_output.csv\")\n",
    "            pass\n",
    "        x = self.pool1(x)\n",
    "        if self.print_activation:\n",
    "            # print(\"Conv1 output shape:\", x.int_repr())\n",
    "            # save_tensor_to_csv_as_matrix(x, \"pool1_output.csv\")\n",
    "            pass\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.reshape(-1, 16 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "def load_quantized_model_and_labels(model_path, num_classes, device):\n",
    "    # Re-create the float model\n",
    "    model = LeNet5(num_classes=num_classes)\n",
    "    model.eval()\n",
    "\n",
    "    # Set the same QAT configuration as before\n",
    "    custom_qconfig = quant.QConfig(\n",
    "        activation=quant.FakeQuantize.with_args(observer=quant.MinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine),\n",
    "        weight=quant.FakeQuantize.with_args(observer=quant.MinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)\n",
    "    )\n",
    "\n",
    "    # Apply the custom QConfig to the model\n",
    "    model.qconfig = custom_qconfig\n",
    "\n",
    "    # Fuse modules just like during training\n",
    "    model_fused = torch.ao.quantization.fuse_modules(\n",
    "        model,\n",
    "        [['conv1', 'relu1'], ['conv2', 'relu2'], ['fc1', 'relu3'], ['fc2', 'relu4']]\n",
    "    )\n",
    "\n",
    "    # Prepare for QAT (simulates the same steps taken during training)\n",
    "    model_prepared = torch.ao.quantization.prepare_qat(model_fused.train(), inplace=True)\n",
    "\n",
    "    # Switch to eval mode before feeding dummy data\n",
    "    model_prepared.eval()\n",
    "\n",
    "    dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model_prepared(dummy_input)  # Populates the observers\n",
    "\n",
    "    # Now convert the model to quantized form after observers have data\n",
    "    model_int8 = torch.ao.quantization.convert(model_prepared.eval())\n",
    "\n",
    "    # Load the saved quantized model weights and labels\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model_int8.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Check if 'labels_mapping' exists in the checkpoint\n",
    "    labels_mapping = checkpoint.get('labels_mapping', None)  # Default to None if key doesn't exist\n",
    "\n",
    "    # Move to device and set to eval mode\n",
    "    model_int8.to(device)\n",
    "    model_int8.eval()\n",
    "\n",
    "    if labels_mapping is not None:\n",
    "        return model_int8, labels_mapping\n",
    "    else:\n",
    "        return model_int8, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6, 5, 5])\n",
      "Num of Channels in Layer 2 is: 96\n",
      "('04142732151018222C1F1E1A2226032310181EFD0710042108FFFFFE8D', 'F8FD07F4F0F7F5FEF1F9F8F7F4F2F5F904F7FE01F0FBF60503FFFFFE8D', '0E0E04FAFD020C0807E90605F5DBF210EEF3F317FCFB020B03FFFFFE8D', '02F601090408FEECF6FE0EF402F7F206F20007FD05FC080813FFFFFE8D', '0B2A2519120F2528181310211D121D0D251218FEF812FE05FDFFFFFE8D', '141101120AF4FE090B00FBFAFB15080BFC010C1003FDF21411FFFFFE8D', '03F4F9CFE706D1D0EDE5D2B1EF07EAE3EA1111011F12030F05FFFFFD1A', 'F7F3EDF2FBF6F8030805090609FBFEFAF8F5FAFAFEFDF0FBFAFFFFFD1A', '17180B00F50BFE190C05F60710090F2B2B1F131709FE02F600FFFFFD1A', '01F70B0901133B382B1416372D14EE09ECF1F1DC0F0A07F7EAFFFFFD1A', 'FFECD7C8E009F3DED20B0ADCD5DF0711F508F7FE1028160D11FFFFFD1A', '0D0C191FEB0815130D05ED00110BF313FFFFEDFFE8F901F1F4FFFFFD1A') Channel: 1\n",
      "('B29ECEC0B7D5CCCABCA10903F0D29B0E1F0903B91B162D16AC000003FF', '0303FE01060407FB08FF050000FC030105FBF80701FA0BF805000003FF', 'D1E3B8C9C51212F7EEC821250FFEE5081504F2E2F7090A0DF9000003FF', '05FF0A080DF7F6F1E7E50805F7E8DE06050AF9E60205FAF9F4000003FF', '99C8C5CCD1A3E9F6DFECE60512F607220A30FA16F3022D2323000003FF', '0CF713050D10FCFFFBFEFB1119F8281119262F0003250E24EF000003FF', '0404C3AC03160BF2BDFC0A1AF1D301F11BF8DD16F01EF5E014FFFFFE92', 'E9E9F80C09EAECFBF6F7F0E901F9FFF0F0F7EEF9E8E9F6EFEBFFFFFE92', 'F6F2F9F5F90616171605FC180D15FE1527211A14F8090405FBFFFFFE92', '04FE0D1421190FF4C8EDEFF809F8F4FEF309080A0CFCF4EFF4FFFFFE92', '1A0614EFF113372E2A0A0C1E2412150422331A1BEE1D3E200CFFFFFE92', 'FE0F03F010100E261CED0B08011218E80AFE0DF90509D105F5FFFFFE92') Channel: 13\n",
      "('061CE8D0F719FADFEDFD000610100C0030160207FCFADEE106FFFFFF8B', '0AF8FCFA0006F907F7F8F9FBF1F3F8FDF6F8F4FFFDFF020702FFFFFF8B', '151AFBFD06000216231D222D362910F4FAEF0DF6EBEEFE0B09FFFFFF8B', '03FF08FFFCFA0901F405E4D1C2F507E6F5010C070D1B221C20FFFFFF8B', 'FBF6E4E5EEFEF6EEF71512060F151E0E141C1A10FCF7ECEFE4FFFFFF8B', 'F8FBE9F3F2DFE0FA0DFAEEF6E9F805030C0E0F1502F5010611FFFFFF8B', 'E8F7000CFFECE0130722FB2226FB1BCEFDD5EDE2F6EFD4E0D5000000A7', 'F8FFFAF806FAFD0104FA020002F7F5FB030A070206030D0911000000A7', 'F6EFF0EE0B08111D1E1FE4F5FE0402A4C5DCC8B6120C070502000000A7', 'FFFD0BFBFFFBFA02E8EA10100D040F030A1D1416E2EAFEFDFC000000A7', 'ECF4FEEB0516061E1B0D06070D1702F5E801CDC002FB0FECF2000000A7', 'ECE2EDEBEF02151707E925171312FE260806F9F2FBF9F1EBEE000000A7') Channel: 25\n",
      "('E819402A10E8F42A1707E21227210FB3122C2B0DE52A312E0EFFFFFB74', 'FDF9E7E9DCF2F5EEEAECF6F7E5F4EAF2FAEAF6EFE8E7E3F1EDFFFFFB74', '02F6E9F1FB0BFA0006F406FB0C16F71D191B0BF40A03F308FEFFFFFB74', '050A091004FB09101C2B0C0BF9FC0306FC010BFD0A0203EB09FFFFFB74', '05E6EE0215FEFAED0F1AFCF4FF212305F1F91E220A0C143023FFFFFB74', '040E1217200F0D111E120E101F190F0B0817FBFB1413FAFD0CFFFFFB74', 'F2FC052002EE0200040AF80AFC1518E5E6F0F612EB0603EF19FFFFFC63', 'F0F7FAFF05EFFE02F7F7E8F6F7F7F9EFFFFFFAF5F5F6E8ECF1FFFFFC63', '0AFF110FFBEC1111130FDC03090B02DDFD1D0D03FB0005140BFFFFFC63', 'FAFF0C010819040313FD0502ECFCF70216F1F0FB03201500F1FFFFFC63', '382D0EEBFC1B3A10F7F6044211F6EBF516120206F406040006FFFFFC63', '0DFBF9FCE8391412F0F32D2F270E0A49554B00DD263B2F36EDFFFFFC63') Channel: 37\n",
      "('BCE205C203A2BB81B8E7A7AEA8EFF1EEE5FA1311051204060A000004B7', '0A09050108FCFD02040505FD080303F90406090603090802FE000004B7', 'C3E201E9D5D8E3C7F011EFE9000918EE01162D37F8F3F30C01000004B7', '17060F0B0AEEE6F5DBD6F2E4F5D8EDF4E4E5F118F9F9140B15000004B7', 'DCE1E1E99EDADFBBCEBDEFE9E0D7ECEEFBE5FE1EF4EE00FB16000004B7', '05091103F2E1DBE2A3CEC6CCB5BFF4E3DD04F602E7F3061C17000004B7', '08010E141408E9E9E3E7F0CEB5C3F700F7CB0113042415FAFAFFFFFF2E', '06060700FBFBFD0A05FB08FF04FD0903FA03F8FCEDEAF8F5EFFFFFFF2E', 'FDF5E3EBFF09F8E6D5ED1D1F06EE04313B362820F600FBFE0AFFFFFF2E', 'F4F90306F8F40A01060907F50B191E1D13F0DDE3081A08F2DAFFFFFF2E', 'FAECE0E6ED0AF4CACEDAFBF4F0DDE8FF110E240F000B122E33FFFFFF2E', 'CBE3E7FFF8ECE0E9F1081108EFF7121226392E08F4011A2307FFFFFF2E') Channel: 49\n",
      "('00F904FE02050305FD0601F30408FDF9FB0203FAFCF9FCFAFCFFFFFD72', 'FBF2F902F2F3FDFFF7F301F5F200F3F3FDFC00F1F7F7F5F5FCFFFFFD72', '0207FE02F9FF01FB0501FC0700F6FDFF0600F806FEF6FFFBF8FFFFFD72', '00F6F80006FDFEFC000108F4FEF70003FC04F9F709F3FAF5F9FFFFFD72', 'FDF60607FAFA09FDF6F7FF00F700FEF9FBFE0504FE03F6F5F8FFFFFD72', '04FCFE01FC02FCF801FCF707FF04FCFD03F4FFFAF701FF03F9FFFFFD72', 'D92134FD00BAEBFEF4EFA0A9E5DEEAD8D1DFFAE70B07FFF5E500000384', '010803060201FEFE070400FDFBFB080000FB02F9FD0107040800000384', 'F2F801FAFEC9D3D5F4040AE6DDF910EFFE1405000D15170E0000000384', '0D1312F6FDF2000F0AFFE0EE03FF09D8FBEEFBF517F6E4020800000384', 'E3EDF42603B6A0D6FAEBB9BBDDEFE4E2E0F4F10DF4050F110000000384', '08FB2417FCFDEA0918FBDFC4ED0EFFFFF00202FF0F121206F700000384') Channel: 61\n",
      "('8FC2AFACAABBC6D09A92D80F0DF9E5E516EAFEADE1EFE3D8C000000390', '0A080A020B090508FD00FC03080506080809F9F90B0B0508FF00000390', 'EAC0A9C5C402040AF0EE191F121212FEF3E6F3EC0502FDF6F300000390', '170CF901FD00050603E2D7EFF2E4CE041412FE02FC07FF060600000390', 'E6C6B7DEC7FAE3DD06E8FD04FA210101E307FF06F9F1EFE4F900000390', '14231403F0C8E404E6CFF00EE5F8FCE90B120B0CEAF8FCF5F600000390', '1A02FBFED6170615150807070E1621FB09FC0E070EFAEE07F7FFFFFC83', 'F5F1EBDFE4F8F4FEFDFAFBFDFAF904F6FD000406F2FE040500FFFFFC83', '08F2D2C4AF1A0F17F90A150902E9E301F4EDE7F103EFF6FCF6FFFFFC83', '05FDF80D0E2F403D512B1507F9050F040E03F6F6F404F5F7FFFFFFFC83', 'EEECE9CECC17F0F1090F0D09EE0F1C06120E0D0F13010DFEECFFFFFC83', '060F0D131AF2EC1704311EFEF5000C0510FC0D102A08FA1410FFFFFC83') Channel: 73\n",
      "('EDE4FDEBDDD70E15E6D8C90402DCCCC3F9EFBCB2D0E3F9CCC7000002B0', '0A0C030C08FC08060C0B040806FD000607070C02FAFA0A0601000002B0', 'DEFE1DFFF8DAF907F5F3D2FD05E6E9F4060BFFF7DFF301F9E5000002B0', '07FD070410FC0300F3ECFDFBFE0C00EBF602090DF21404F9FA000002B0', 'B8D608FA05ABC3F801EFC6D5EB04D2DDCF0109C2ECF8E410E9000002B0', '13160C00FB060E0D13F601110E0F081411130EF5151D22FDE7000002B0', '09CBDD2531EE0306310F02352D25110B22140FEE0DF81207F2FFFFFE1D', 'F8FC06FCF503F807F2FCF901F4F8F4F0FBFAF2F6E2EEF4F9F9FFFFFE1D', 'EBE42E3922F70F3029F904262502F2151F060D070E07F8FBFFFFFFFE1D', '00E6B6B3F9EEF3EB312ECCCF011901D7EB071806DCEBFE02FEFFFFFE1D', '151CFF0F08100D1CFA1315121E180F091F17171703301004F5FFFFFE1D', 'FFE5D9ECF6F2D9DFED02F8F2EC0916F5F00802130C03F00000FFFFFE1D') Channel: 85\n"
     ]
    }
   ],
   "source": [
    "def create_coe_files_convs(path, num_classes):\n",
    "    # Example usage (ensure you have defined test_loader and dataset.int_to_char):\n",
    "    model_path = path  # Path to your saved quantized model\n",
    "    device = torch.device(\"cpu\")\n",
    "      # Update with your number of classes\n",
    "\n",
    "    # Input name, Activation Name\n",
    "    names_list = [['conv1','conv2']]\n",
    "    for layer in range(len(names_list)):\n",
    "        # Model parameters\n",
    "        model_int8, labels  = load_quantized_model_and_labels(model_path, num_classes, device)\n",
    "        input_scale = model_int8.state_dict()[f'{names_list[layer][0]}.scale']\n",
    "        input_zero_point = model_int8.state_dict()[f'{names_list[layer][0]}.zero_point']\n",
    "        weight_scale = model_int8.state_dict()[f'{names_list[layer][1]}.weight'].q_scale()\n",
    "        weight_zero_point = model_int8.state_dict()[f'{names_list[layer][1]}.weight'].q_zero_point()\n",
    "        effective_scale = input_scale * weight_scale # Scale for the output\n",
    "\n",
    "        # Creating weights and Biases Lists\n",
    "        active_weights = model_int8.state_dict()[f'{names_list[layer][1]}.weight'].int_repr()\n",
    "        active_biases = model_int8.state_dict()[f'{names_list[layer][1]}.bias']\n",
    "        active_biases_list = active_biases.tolist()\n",
    "        q_biases = []\n",
    "        for i in range(len(active_biases_list)):\n",
    "            q_value = float(active_biases_list[i]) / effective_scale\n",
    "            q_biases.append(round(float(q_value)))\n",
    "\n",
    "        parameters_list = []\n",
    "        # Print weights for each channel with separation\n",
    "        num_output_channels = active_weights.shape[0]  # Number of channels in conv1\n",
    "        num_input_channels = active_weights.shape[1]\n",
    "        print(active_weights.size())\n",
    "        for output_channel in range(num_output_channels):\n",
    "            # Convert the first 25 weights to a list\n",
    "                for input_channel in range(num_input_channels):\n",
    "\n",
    "                    channel_list = active_weights[output_channel][input_channel].reshape(-1)[:25].tolist()\n",
    "                    channel_list.append(q_biases[output_channel])  # Append the bias of the current channel\n",
    "                    parameters_list.append(channel_list)\n",
    "\n",
    "        print(f'Num of Channels in Layer {layer+2} is: {len(parameters_list)}')\n",
    "        # print(len(parameters_list))\n",
    "        counter = 0\n",
    "        for i in range(0,len(parameters_list)-1,12):\n",
    "            # print(parameters_list[i], i)\n",
    "\n",
    "            print(f'{generate_coe_file_hex_with_padding(parameters_list[i], parameters_list[i+1],parameters_list[i+2],parameters_list[i+3],parameters_list[i+4],parameters_list[i+5],parameters_list[i+6], parameters_list[i+7],parameters_list[i+8],parameters_list[i+9],parameters_list[i+10],parameters_list[i+11],f'Layer{layer+2}_Channel{counter+1}_{counter+2}.coe')} Channel: {i+1}')\n",
    "            counter = counter + 2\n",
    "\n",
    "create_coe_files_convs('lenetV5.pth', 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to_signed( 0 , 8),', 'to_signed( 1 , 8),', 'to_signed( 2 , 8),', 'to_signed( 3 , 8),', 'to_signed( 4 , 8),', 'to_signed( 5 , 8),', 'to_signed( 6 , 8),', 'to_signed( 7 , 8),', 'to_signed( 8 , 8),', 'to_signed( 9 , 8),', 'to_signed( 10 , 8),', 'to_signed( 11 , 8),', 'to_signed( 12 , 8),', 'to_signed( 13 , 8),', 'to_signed( 14 , 8),', 'to_signed( 15 , 8),', 'to_signed( 16 , 8),', 'to_signed( 17 , 8),', 'to_signed( 18 , 8),', 'to_signed( 19 , 8),', 'to_signed( 20 , 8),', 'to_signed( 21 , 8),', 'to_signed( 22 , 8),', 'to_signed( 23 , 8),', 'to_signed( 24 , 8),', 'to_signed( 25 , 8),', 'to_signed( 26 , 8),', 'to_signed( 27 , 8),', 'to_signed( 28 , 8),', 'to_signed( 29 , 8),', 'to_signed( 30 , 8),', 'to_signed( 31 , 8),', 'to_signed( 32 , 8),', 'to_signed( 33 , 8),', 'to_signed( 34 , 8),', 'to_signed( 35 , 8),', 'to_signed( 36 , 8),', 'to_signed( 37 , 8),', 'to_signed( 38 , 8),', 'to_signed( 39 , 8),', 'to_signed( 40 , 8),', 'to_signed( 41 , 8),', 'to_signed( 42 , 8),', 'to_signed( 43 , 8),', 'to_signed( 44 , 8),', 'to_signed( 45 , 8),', 'to_signed( 46 , 8),', 'to_signed( 47 , 8),', 'to_signed( 48 , 8),', 'to_signed( 49 , 8),', 'to_signed( 50 , 8),', 'to_signed( 51 , 8),', 'to_signed( 52 , 8),', 'to_signed( 53 , 8),', 'to_signed( 54 , 8),', 'to_signed( 55 , 8),', 'to_signed( 56 , 8),', 'to_signed( 57 , 8),', 'to_signed( 58 , 8),', 'to_signed( 59 , 8),', 'to_signed( 60 , 8),', 'to_signed( 61 , 8),', 'to_signed( 62 , 8),', 'to_signed( 63 , 8),', 'to_signed( 64 , 8),', 'to_signed( 65 , 8),', 'to_signed( 66 , 8),', 'to_signed( 67 , 8),', 'to_signed( 68 , 8),', 'to_signed( 69 , 8),', 'to_signed( 70 , 8),', 'to_signed( 71 , 8),', 'to_signed( 72 , 8),', 'to_signed( 73 , 8),', 'to_signed( 74 , 8),', 'to_signed( 75 , 8),', 'to_signed( 76 , 8),', 'to_signed( 77 , 8),', 'to_signed( 78 , 8),', 'to_signed( 79 , 8),', 'to_signed( 80 , 8),', 'to_signed( 81 , 8),', 'to_signed( 82 , 8),', 'to_signed( 83 , 8),', 'to_signed( 84 , 8),', 'to_signed( 85 , 8),', 'to_signed( 86 , 8),', 'to_signed( 87 , 8),', 'to_signed( 88 , 8),', 'to_signed( 89 , 8),', 'to_signed( 90 , 8),', 'to_signed( 91 , 8),', 'to_signed( 92 , 8),', 'to_signed( 93 , 8),', 'to_signed( 94 , 8),', 'to_signed( 95 , 8),', 'to_signed( 96 , 8),', 'to_signed( 97 , 8),', 'to_signed( 98 , 8),', 'to_signed( 99 , 8),', 'to_signed( 100 , 8),', 'to_signed( 101 , 8),', 'to_signed( 102 , 8),', 'to_signed( 103 , 8),', 'to_signed( 104 , 8),', 'to_signed( 105 , 8),', 'to_signed( 106 , 8),', 'to_signed( 107 , 8),', 'to_signed( 108 , 8),', 'to_signed( 109 , 8),', 'to_signed( 110 , 8),', 'to_signed( 111 , 8),', 'to_signed( 112 , 8),', 'to_signed( 113 , 8),', 'to_signed( 114 , 8),', 'to_signed( 115 , 8),', 'to_signed( 116 , 8),', 'to_signed( 117 , 8),', 'to_signed( 118 , 8),', 'to_signed( 119 , 8),']\n"
     ]
    }
   ],
   "source": [
    "numbers = range(0,120)  # Replace the ellipsis with the actual numbers or generate them\n",
    "prefix = \"to_signed(\"    # The text you want before the number\n",
    "suffix = \", 8),\"   # The text you want after the number\n",
    "\n",
    "# Generate the new list with text added on both sides of each number\n",
    "modified_numbers = [f\"{prefix} {number} {suffix}\" for number in numbers]\n",
    "\n",
    "# Print or use the modified numbers as needed\n",
    "print(modified_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    to_signed( -10, 8),\n",
      "    to_signed(  21, 8),\n",
      "    to_signed(  26, 8),\n",
      "    to_signed(-106, 8),\n",
      "    to_signed(  -7, 8),\n",
      "    to_signed( -40, 8),\n",
      "    to_signed(  28, 8),\n",
      "    to_signed(  43, 8),\n",
      "    to_signed(  18, 8),\n",
      "    to_signed( -29, 8),\n",
      "    to_signed( -73, 8),\n",
      "    to_signed(   5, 8),\n",
      "    to_signed(  54, 8),\n",
      "    to_signed(  38, 8),\n",
      "    to_signed(  32, 8),\n",
      "    to_signed(-106, 8),\n",
      "    to_signed(  26, 8),\n",
      "    to_signed(  82, 8),\n",
      "    to_signed(  83, 8),\n",
      "    to_signed(-125, 8),\n",
      "    to_signed( -40, 8),\n",
      "    to_signed(  10, 8),\n",
      "    to_signed( 103, 8),\n",
      "    to_signed( -25, 8),\n",
      "    to_signed( -89, 8),\n"
     ]
    }
   ],
   "source": [
    "numbers = [-10, 21, 26, -106, -7, -40, 28, 43, 18, -29, -73, 5, 54, 38, 32, -106, 26, 82, 83, -125, -40, 10, 103, -25, -89]  # Example numbers\n",
    "\n",
    "# The function call format\n",
    "formatted_numbers = [f\"    to_signed({number:4d}, 8),\" for number in numbers]\n",
    "\n",
    "# Print each formatted line\n",
    "for line in formatted_numbers:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample images have been copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def copy_sample_images(\n",
    "    source_directory,\n",
    "    destination_directory='image_testing',\n",
    "    images_per_folder=15\n",
    "):\n",
    "    \"\"\"\n",
    "    Copies a sample of images (default 15) from each label folder in the source_directory\n",
    "    into a new 'image_testing' folder with matching label subfolders.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the destination top-level folder exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Iterate over each label folder in the source\n",
    "    for label_folder in os.listdir(source_directory):\n",
    "        label_path = os.path.join(source_directory, label_folder)\n",
    "\n",
    "        # Ensure we only process directories\n",
    "        if os.path.isdir(label_path):\n",
    "            # Create the corresponding label folder under 'image_testing'\n",
    "            dest_label_path = os.path.join(destination_directory, label_folder)\n",
    "            os.makedirs(dest_label_path, exist_ok=True)\n",
    "\n",
    "            # List all image files in the label folder\n",
    "            images = [\n",
    "                f for f in os.listdir(label_path)\n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif'))\n",
    "            ]\n",
    "\n",
    "            # Sort or randomize the image list\n",
    "            images.sort()\n",
    "            # For a random sample instead of the first 15, uncomment the next line:\n",
    "            random.shuffle(images)\n",
    "\n",
    "            # Select the first N images (default 15)\n",
    "            selected_images = images[:images_per_folder]\n",
    "\n",
    "            # Copy selected images\n",
    "            for img in selected_images:\n",
    "                source_img = os.path.join(label_path, img)\n",
    "                dest_img = os.path.join(dest_label_path, img)\n",
    "                shutil.copyfile(source_img, dest_img)\n",
    "\n",
    "    print(\"Sample images have been copied successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update the path to your 33-label-folder directory\n",
    "    source_directory = \"D:/pynq/FinalProject/dataset\"\n",
    "    # Optionally, update the destination directory name if desired\n",
    "    destination_directory = \"image_testing\"\n",
    "\n",
    "    copy_sample_images(source_directory, destination_directory, images_per_folder=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time over 1000 runs: 0.2627084255218506 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Set environment variables BEFORE importing torch or any NumPy/Scipy libs\n",
    "# ------------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"   # Controls number of threads for OpenMP\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"   # Controls number of threads for MKL (used by NumPy/PyTorch)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"  # If you use numexpr\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # If you use OpenBLAS\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # If you use Apple's Accelerate/vecLib\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Set the number of threads within PyTorch\n",
    "# ------------------------------------------------------------------------------\n",
    "torch.set_num_threads(1)\n",
    "# torch.set_num_interop_threads(1)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 33)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.reshape(-1, 16 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Force CPU for timing simplicity\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Instantiate the model and move to CPU\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "runs = 1000\n",
    "sum_total_time = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(runs):\n",
    "        # Create a new random input for each run\n",
    "        dummy_input = torch.randn((1, 1, 28, 28), device=device, dtype=torch.float32)\n",
    "\n",
    "        start_time = time.time()\n",
    "        _ = model(dummy_input)\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        sum_total_time += end_time - start_time\n",
    "\n",
    "avg_time_per_run = sum_total_time / runs\n",
    "print(f\"Average inference time over {runs} runs: {avg_time_per_run * 1000} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting: 100%|██████████| 634/634 [00:00<00:00, 1642.66img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Done.  Inverted 634 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Batch colour inversion ---------------------------------------------------\n",
    "# 1. Change SRC_DIR below to the folder that contains your images (or sub-folders)\n",
    "# 2. Set INPLACE = True  if you want to overwrite the originals  (⚠ irreversible)\n",
    "#    Set INPLACE = False if you want all inverted images written to SRC_DIR+\"_inverted\"\n",
    "# 3. Run the cell\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.auto import tqdm   # progress bar\n",
    "\n",
    "# ------------------------------------------------------------------ settings --\n",
    "SRC_DIR  = r\"D:/FinalProject/dataset/=\"   # <----- put your folder here\n",
    "INPLACE  = False                          # True = overwrite originals\n",
    "RECURSE  = True                           # descend into sub-folders too?\n",
    "\n",
    "# List of accepted image extensions (lowercase)\n",
    "VALID_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "# ----------------------------------------------------------------- functions --\n",
    "def invert_image_file(src_path: Path, dst_path: Path):\n",
    "    \"\"\"Load one image, invert colours, save to dst_path.\"\"\"\n",
    "    with Image.open(src_path) as im:\n",
    "        if im.mode not in (\"L\", \"RGB\"):\n",
    "            im = im.convert(\"RGB\")\n",
    "        inv = ImageOps.invert(im)\n",
    "        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        inv.save(dst_path)\n",
    "\n",
    "# -------------------------------------------------------------------- run ----\n",
    "src_path = Path(SRC_DIR).expanduser().resolve()\n",
    "dst_root = src_path if INPLACE else src_path.with_name(src_path.name + \"_inverted\")\n",
    "\n",
    "pattern  = \"**/*\" if RECURSE else \"*\"\n",
    "files    = [p for p in src_path.glob(pattern) if p.suffix.lower() in VALID_EXTS]\n",
    "\n",
    "if not files:\n",
    "    print(\"⚠️  No images found – check path or extensions.\")\n",
    "else:\n",
    "    for f in tqdm(files, desc=\"Inverting\", unit=\"img\"):\n",
    "        dst = f if INPLACE else dst_root / f.relative_to(src_path)\n",
    "        invert_image_file(f, dst)\n",
    "\n",
    "    print(\"✅  Done.  Inverted\", len(files), \"images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Padded images saved to: D:\\FinalProject\\dataset\\padded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pad every image in IN_DIR with a uniform black border.\n",
    "\n",
    "Works for white glyphs on a black background.\n",
    "If your colours are inverted, set INVERT = True.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2          # pip install opencv-python\n",
    "import numpy as np\n",
    "\n",
    "# ─── USER SETTINGS ────────────────────────────────────────────────────────────\n",
    "IN_DIR   = Path(r\"D:/FinalProject/dataset/pi\")   # ← CHANGE ME\n",
    "PAD      = 15        # pixels of padding you want on each side\n",
    "MAKE_SQUARE = True  # keep aspect ratio but pad until width == height\n",
    "RESIZE_TO   = (28, 28)      # set to None to skip resizing\n",
    "INVERT      = True         # True if your glyphs are black on white\n",
    "THICKEN = 2      # how many pixels to grow each stroke\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "OUT_DIR = IN_DIR.parent / \"padded\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def pad_one(img_gray: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1.  Crop to the glyph\n",
    "    2.  Thicken the strokes via morphological dilation\n",
    "    3.  Paste into a centred black canvas (with padding / optional square)\n",
    "    4.  Optionally resize\n",
    "    \"\"\"\n",
    "    # ensure glyph is white (255) on black (0)\n",
    "    fg = 255 - img_gray if INVERT else img_gray\n",
    "\n",
    "    # binary mask of the glyph\n",
    "    _, mask = cv2.threshold(fg, 0, 255,\n",
    "                            cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # ─── 2. THICKEN ───────────────────────────────────────────────────────────\n",
    "    if THICKEN > 0:\n",
    "        k = 2 * THICKEN + 1                       # e.g. 2-px → 5×5 kernel\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "        mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    ys, xs = np.where(mask)\n",
    "    if ys.size == 0:                              # empty -> return original\n",
    "        return img_gray\n",
    "\n",
    "    # crop to tight bounding box\n",
    "    y0, x0, y1, x1 = ys.min(), xs.min(), ys.max(), xs.max()\n",
    "    glyph = mask[y0:y1 + 1, x0:x1 + 1]            # binary after thickening\n",
    "    glyph = (glyph > 0).astype(np.uint8) * 255    # back to 0/255\n",
    "\n",
    "    # ─── 3. centre on a canvas with padding ──────────────────────────────────\n",
    "    h, w = glyph.shape\n",
    "    if MAKE_SQUARE:\n",
    "        side = max(h, w) + 2 * PAD\n",
    "        canvas = np.zeros((side, side), dtype=np.uint8)\n",
    "        y_off = (side - h) // 2\n",
    "        x_off = (side - w) // 2\n",
    "    else:\n",
    "        canvas = np.zeros((h + 2 * PAD, w + 2 * PAD), dtype=np.uint8)\n",
    "        y_off = PAD\n",
    "        x_off = PAD\n",
    "\n",
    "    canvas[y_off:y_off + h, x_off:x_off + w] = glyph\n",
    "\n",
    "    # ─── 4. resize if requested ──────────────────────────────────────────────\n",
    "    if RESIZE_TO is not None:\n",
    "        canvas = cv2.resize(canvas, RESIZE_TO, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return canvas\n",
    "\n",
    "\n",
    "# ─── MAIN LOOP ────────────────────────────────────────────────────────────────\n",
    "img_exts = (\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.bmp\")\n",
    "for ext in img_exts:\n",
    "    for img_path in IN_DIR.glob(ext):\n",
    "        gray = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if gray is None:\n",
    "            print(f\"⚠️  Skipping {img_path.name}: not an image\")\n",
    "            continue\n",
    "\n",
    "        padded_img = pad_one(gray)\n",
    "        cv2.imwrite(str(OUT_DIR / img_path.name), padded_img)\n",
    "\n",
    "print(f\"✅ Done! Padded images saved to: {OUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm               # progress bar – optional\n",
    "\n",
    "CSV_PATH  = Path(\"D:/FinalProject/dataset/digits.csv\")   # 699 MB file\n",
    "DEST_ROOT = Path(\"D:/FinalProject/dataset/AZ_images\")                  # will hold A…Z sub-folders\n",
    "\n",
    "# Map 0-25 ➜ 'A'-'Z'\n",
    "label_to_char = {i: chr(i + 65) for i in range(26)}\n",
    "\n",
    "# read in manageable chunks (keeps RAM <3 GB even on 16 GB laptops)\n",
    "for chunk in pd.read_csv(CSV_PATH, header=None, chunksize=50_000):\n",
    "    labels  = chunk.iloc[:, 0].astype(int).to_numpy()\n",
    "    pixels  = chunk.iloc[:, 1:].to_numpy(dtype=np.uint8)\n",
    "\n",
    "    for idx, (lab, flat) in enumerate(zip(labels, pixels)):\n",
    "        img = flat.reshape(28, 28)           # 784 ➜ 28 × 28 matrix\n",
    "        char_dir = DEST_ROOT / label_to_char[lab]\n",
    "        char_dir.mkdir(parents=True, exist_ok=True)\n",
    "        Image.fromarray(img, mode=\"L\").save(char_dir / f\"{lab}_{idx:06d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
